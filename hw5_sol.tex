% Original Latex template author:
% Frits Wenneker (http://www.howtotex.com)
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{graphicx}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\usepackage{lettrine,setspace,enumerate}
\usepackage[margin=.5in]{geometry}
\setlength{\parindent}{0cm}
%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{ 
\normalfont \normalsize 
%\textsc{Arbitrary Labs} \\ [5pt]
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CS 189 HW 5 \\ % The assignment title
\horrule{2pt} \\[-0.1cm] % Thick bottom horizontal rule
}

\author{Eddie Groshev, Rishi Sharma, Connor Braa}

\begin{document}
    
\maketitle % Print the title

%----------------------------------------------------------------------------------------
%   BEGIN DOCUMENT
%----------------------------------------------------------------------------------------

% -------------------- PART 1 --------------------
{\Large 1. \indent Implementation and Features}
\\
\begin{enumerate}[(1)]

% 1 (a)
\item Decision Tree
\begin{enumerate}[(a)]
\item A robust implementation with multiple options avaliable.
\begin{enumerate}[(i)]
\item Allows for depth limiting.
\item Allows the use of different impurity metrics, the following are implemented:
\begin{enumerate}[-]
\item Entropy impurity
\item Gini impurity
\item Misclassification impurity
\end{enumerate}
\item Allows for reduced-error pruning.
\item Allows the user to explicitly pass in a subset of features to be used for training.
\item Has functions to make bagging and random forests easier to implement.
\end{enumerate}
\end{enumerate}


\item Random Forest
\begin{enumerate}[(a)]
\item Builds on the decision tree implementation, and trains M different decision trees.
\begin{enumerate}[(i)]
\item Can build a forest based on the bagging model where each tree is trained on a random subset of the original data.
\item Can build a forest where each tree is built on a random subset of the features.
\begin{enumerate}[-]
\item Can be expanded to split at each node based on a random subset of the features.
\item Can also be mixed with the bagging model do decorrelate the tree's even more.
\end{enumerate}
\end{enumerate}
\end{enumerate}

\item AdaBoost
\begin{enumerate}[(a)]
\item Instead of rewriting each classifier to account for weights, we just sample the dataset using the weighted distribution.
\end{enumerate}

\end{enumerate}

\newpage % -------------------- PART 2 --------------------
 {\Large 2. \indent Results and Analysis}
\\

\begin{enumerate}[(a)]
\item 10-fold cross validation for decision trees was around 9.4\% error.
\begin{enumerate}[-]
\item Limiting the depth to about 20 gave 0.5\% improvement.
\item This version of pruning didn't seem to give much improvement.
\item Gini impurity and Entropy gave almost exactly the same results, which makes sense because those two functions are very similar. 
\item Misclassification impurity performed worse than the other two impurity measures.
\end{enumerate}
\item 10-fold cross validation for random forests is in the range of 6.2\%6.8\% error.
\begin{enumerate}[-]
\item Surprisingly, splitting on 17 randomly chosen features was better than any other value.
\item Randomly sampling the data vs sampling both the features and the data showed no significant difference during cross validation.
\end{enumerate}
\item 10-fold cross validation for adaboost (4 itterations) is around 12.4\%
\end{enumerate}
$\\
\\
\\$

 {\Large 3. \indent Resources and Extra Info}
\\

\begin{enumerate}[(i)]
\item We used the pdf files that came with the hw, along with some examples from the scipy api for cross validation.
\item The libraries we used are all described in README.txt
\end{enumerate}

\end{document}